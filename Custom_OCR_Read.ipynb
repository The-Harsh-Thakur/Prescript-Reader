{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "50af9718-b983-46c7-af7d-cf96be263b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import load_model\n",
    "from mltu.configs import BaseModelConfigs\n",
    "from tqdm import tqdm\n",
    "from mltu.utils.text_utils import ctc_decoder, get_cer\n",
    "from mltu.inferenceModel import OnnxInferenceModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb5e882a-ed9c-4640-889d-2c213736e82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom OCR model\n",
    "class ImageToWordModel(OnnxInferenceModel):\n",
    "    def __init__(self, char_list, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.char_list = char_list\n",
    "\n",
    "    def predict(self, image):\n",
    "        image = cv2.resize(image, self.input_shapes[0][1:3][::-1])\n",
    "        image_pred = np.expand_dims(image, axis=0).astype(np.float32)\n",
    "        preds = self.model.run(self.output_names, {self.input_names[0]: image_pred})[0]\n",
    "        text = ctc_decoder(preds, self.char_list)[0]\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "657bdc4f-0532-4e51-8b0b-b50ee4e8e91e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained CNN model and make predictions\n",
    "def tester(model, img):\n",
    "    img_resized = cv2.resize(img, (128, 128))  # Resize to the input size of the CNN\n",
    "    img_resized = img_resized.astype('float32') / 255.0  # Normalize the image\n",
    "    img_resized = np.expand_dims(img_resized, axis=0)  # Add batch dimension (1, 128, 128, 3)\n",
    "\n",
    "    prediction = model.predict(img_resized)\n",
    "    predicted_class = np.argmax(prediction, axis=1)[0]  # Get class index\n",
    "    class_labels = ['Handwritten', 'Printed']\n",
    "    print(f\"Predicted class: {class_labels[predicted_class]}\")\n",
    "    return class_labels[predicted_class]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c7a9359b-904e-442f-bb88-a7b60da47aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom OCR function using ImageToWordModel\n",
    "def perform_ocr(img, ocr_model):\n",
    "    prediction_text = ocr_model.predict(img)\n",
    "    print(f\"Recognized Handwritten Text: {prediction_text}\")\n",
    "    return prediction_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9539ed24-0c42-4e66-ace7-3eeb229b4a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to clear the output directory\n",
    "def clear_output_directory(output_dir):\n",
    "    if os.path.exists(output_dir):\n",
    "        shutil.rmtree(output_dir)\n",
    "    os.makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32f6775b-0627-4296-886d-aa5ef1e2e452",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classification of Prescription Image Sections\n",
    "def Classify(filename):\n",
    "    output_dir = 'output'\n",
    "    clear_output_directory(output_dir)\n",
    "\n",
    "    img = cv2.imread(filename)\n",
    "    hgt, wdt = img.shape[:2]\n",
    "    hBw = hgt / float(wdt)\n",
    "    dim = (576, int(576 * hBw))\n",
    "    \n",
    "    img = cv2.resize(img, dim)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    linek = np.zeros((11, 11), dtype=np.uint8)\n",
    "    linek[5, ...] = 1\n",
    "    x = cv2.morphologyEx(gray, cv2.MORPH_OPEN, linek, iterations=1)\n",
    "    gray -= x\n",
    "\n",
    "    ret2, gray = cv2.threshold(gray, 10, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    kernel = np.ones((5, 5), np.uint8)\n",
    "    gray = cv2.dilate(gray, kernel, iterations=1)\n",
    "\n",
    "    contours2, _ = cv2.findContours(gray, cv2.RETR_CCOMP, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    cnn_model = load_model(\"cnn_model_images.h5\")  # Load the trained CNN model\n",
    "\n",
    "    configs = BaseModelConfigs.load(\"Models/03_handwriting_recognition/202301111911/configs.yaml\")\n",
    "    ocr_model = ImageToWordModel(model_path=configs.model_path, char_list=configs.vocab)\n",
    "\n",
    "    file_counter = 1\n",
    "    for cnt in contours2:\n",
    "        start_x, start_y, width, height = cv2.boundingRect(cnt)\n",
    "        mymat = img[start_y:start_y + height, start_x:start_x + width]\n",
    "        dect = tester(cnn_model, mymat)\n",
    "\n",
    "        if dect == 'Handwritten':\n",
    "            ocr_result = perform_ocr(mymat, ocr_model)\n",
    "            \n",
    "            sanitized_text = \"\".join([c if c.isalnum() else \"_\" for c in ocr_result]) if ocr_result else \"unrecognized\"\n",
    "            region_filename = f\"{output_dir}/{sanitized_text[:30]}_{file_counter}.png\"\n",
    "            \n",
    "            cv2.imwrite(region_filename, mymat)\n",
    "            print(f\"Saved {region_filename}\")\n",
    "            file_counter += 1\n",
    "        \n",
    "        color = (0, 255, 0) if dect == 'Handwritten' else (255, 0, 0)\n",
    "        cv2.rectangle(img, (start_x, start_y), (start_x + width, start_y + height), color, 2)\n",
    "    \n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(img_rgb)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    cv2.imwrite(\"output/classified_image.png\", img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "604be9ed-1e2a-4486-b5b4-5d3253450509",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n",
      "E0000 00:00:1731603782.215527     350 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "E0000 00:00:1731603782.225101     350 cuda_dnn.cc:522] Loaded runtime CuDNN library: 9.1.0 but source was compiled with: 9.3.0.  CuDNN library needs to have matching major version and equal or higher minor version. If using a binary install, upgrade your CuDNN library.  If building from sources, make sure the library loaded at runtime is compatible with the version specified during compile configuration.\n",
      "2024-11-14 22:33:02.228557: W tensorflow/core/framework/op_kernel.cc:1841] OP_REQUIRES failed at xla_ops.cc:577 : FAILED_PRECONDITION: DNN library initialization failed. Look at the errors above for more details.\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_95/1001296312.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_95/1521166075.py\", line 32, in Classify\n\n  File \"/tmp/ipykernel_95/1140194724.py\", line 7, in tester\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 510, in predict\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 208, in one_step_on_data_distributed\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_data_distributed_777]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Execution - Classify a New Image\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 3\u001b[0m     \u001b[43mClassify\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTemplates/7.jpg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[21], line 32\u001b[0m, in \u001b[0;36mClassify\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     30\u001b[0m start_x, start_y, width, height \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mboundingRect(cnt)\n\u001b[1;32m     31\u001b[0m mymat \u001b[38;5;241m=\u001b[39m img[start_y:start_y \u001b[38;5;241m+\u001b[39m height, start_x:start_x \u001b[38;5;241m+\u001b[39m width]\n\u001b[0;32m---> 32\u001b[0m dect \u001b[38;5;241m=\u001b[39m \u001b[43mtester\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcnn_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmymat\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dect \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHandwritten\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     35\u001b[0m     ocr_result \u001b[38;5;241m=\u001b[39m perform_ocr(mymat, ocr_model)\n",
      "Cell \u001b[0;32mIn[18], line 7\u001b[0m, in \u001b[0;36mtester\u001b[0;34m(model, img)\u001b[0m\n\u001b[1;32m      4\u001b[0m img_resized \u001b[38;5;241m=\u001b[39m img_resized\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m  \u001b[38;5;66;03m# Normalize the image\u001b[39;00m\n\u001b[1;32m      5\u001b[0m img_resized \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img_resized, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Add batch dimension (1, 128, 128, 3)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m prediction \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_resized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m predicted_class \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(prediction, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Get class index\u001b[39;00m\n\u001b[1;32m      9\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHandwritten\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrinted\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mFailedPreconditionError\u001b[0m: Graph execution error:\n\nDetected at node StatefulPartitionedCall defined at (most recent call last):\n  File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n\n  File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 205, in start\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n\n  File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n\n  File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n\n  File \"/tmp/ipykernel_95/1001296312.py\", line 3, in <module>\n\n  File \"/tmp/ipykernel_95/1521166075.py\", line 32, in Classify\n\n  File \"/tmp/ipykernel_95/1140194724.py\", line 7, in tester\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 510, in predict\n\n  File \"/home/harsh/.local/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py\", line 208, in one_step_on_data_distributed\n\nDNN library initialization failed. Look at the errors above for more details.\n\t [[{{node StatefulPartitionedCall}}]] [Op:__inference_one_step_on_data_distributed_777]"
     ]
    }
   ],
   "source": [
    "# Execution - Classify a New Image\n",
    "if __name__ == \"__main__\":\n",
    "    Classify(\"Templates/7.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa3975b-6233-4f60-9276-4c55e3b34b72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
